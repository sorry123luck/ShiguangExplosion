import re
import cv2
import time
import numpy as np
import pytesseract
import threading
import easyocr
import torch 
import global_state as gs
from utils.toast_notify import show_toast
from utils.adb_tools import ScreenshotSocket
from position_config import RESEARCH_POINTS
from tech_timer_manager import (
    set_research_timer, set_accelerate_timer,
    start_timer_thread, research_time_remaining, accelerate_cd_remaining
)

_pause_callback = None


def register_main_callbacks(pause_func):
    global _pause_callback
    _pause_callback = pause_func

# çŠ¶æ€å˜é‡
research_running = False
server_socket = None
research_ready_event = threading.Event()
has_requested_help = False

# æ¨¡æ¿è·¯å¾„
IMG_DONE_PATH = "icons/KJ-YJWC.png"
IMG_ACCEL_TRUE_PATH = "icons/KJ-JSSJ-T.png"
IMG_ACCEL_FALSE_PATH = "icons/YJ-JSSJ-F.png"

try:
    gpu_available = torch.cuda.is_available()
except Exception:
    gpu_available = False

# å¦‚æœ CPU ç‰ˆï¼Œå¼ºåˆ¶è®¾ç½® Falseï¼Œé˜²æ­¢ç‰ˆæœ¬å·é‡Œå‡ºç° +cpu è¯¯åˆ¤
if "+cpu" in torch.__version__:
    gpu_available = False

print(f"ğŸš€ EasyOCR åˆå§‹åŒ–ï¼ŒGPU å¯ç”¨: {gpu_available}")
reader = easyocr.Reader(['ch_sim'], gpu=gpu_available)

# === å·¥å…·å‡½æ•° ===
def get_screenshot():
    sock = ScreenshotSocket("127.0.0.1", 6101)
    data = sock.request_screenshot()
    if not data:
        return None
    arr = np.frombuffer(data, dtype=np.uint8)
    return cv2.imdecode(arr, cv2.IMREAD_COLOR)

def safe_get_screenshot(retries=3, delay=1.0):
    for i in range(retries):
        img = get_screenshot()
        if img is not None:
            if i == 0:
                # åªä¿å­˜ç¬¬ä¸€æ¬¡æˆåŠŸçš„æˆªå›¾ï¼Œé˜²æ­¢å¤šæ¬¡è¦†ç›–
                #cv2.imwrite("tech_current_screen.png", img)
                #print("âœ… å·²ä¿å­˜ç§‘æŠ€ç•Œé¢æˆªå›¾ tech_current_screen.png")
                pass
            return img
        else:
            print(f"âš ï¸ ç¬¬ {i + 1} æ¬¡æˆªå›¾å¤±è´¥ï¼Œç­‰å¾… {delay}s é‡è¯•...")
            time.sleep(delay)
    print("âŒ è¿ç»­æˆªå›¾å¤±è´¥ï¼Œè¿”å› None")
    return None

def crop(img, region):
    x1, y1, x2, y2 = region
    return img[y1:y2, x1:x2]

def extract_time(region_img):
    text = pytesseract.image_to_string(region_img, lang='chi_sim')
    match = re.search(r"(\d{1,2}):(\d{1,2}):(\d{1,2})", text)
    if match:
        h, m, s = map(int, match.groups())
        return h * 3600 + m * 60 + s
    return None

def match_template(region_img, template_path):
    template = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)
    gray = cv2.cvtColor(region_img, cv2.COLOR_BGR2GRAY)
    res = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF_NORMED)
    _, max_val, _, _ = cv2.minMaxLoc(res)
    return max_val > 0.8

def is_research_done(img):
    region = crop(img, RESEARCH_POINTS["ç ”ç©¶å®Œæˆæç¤ºè¯†åˆ«åŒºåŸŸ"])
    return match_template(region, IMG_DONE_PATH)

def is_currently_researching(img):
    x1, y1, x2, y2 = RESEARCH_POINTS["ç ”ç©¶ä¸­åˆ¤æ–­åŒºåŸŸ"]
    roi = img[y1:y2, x1:x2]
    cv2.imwrite("debug_research_roi.png", roi)

    # 1ï¸âƒ£ åŸå›¾
    print("ğŸ” å°è¯•åŸå›¾è¯†åˆ«ç ”ç©¶çŠ¶æ€...")
    results = reader.readtext(roi)
    print("ğŸ§ª OCRåŸå§‹ç»“æœï¼ˆåŸå›¾ï¼‰:", results)
    for _, text, conf in results:
        text_clean = text.replace(" ", "")
        if any(k in text_clean for k in ["ç ”ç©¶", "ç ”", "ç©¶"]) and conf > 0.08:
            print(f"ğŸ“˜ ã€ç ”ç©¶çŠ¶æ€ã€‘è¯†åˆ«æˆåŠŸ (åŸå›¾): {text}")
            return True

    # 2ï¸âƒ£ variant9 é™äº®åº¦
    variant9 = cv2.convertScaleAbs(roi, alpha=0.5, beta=0)
    cv2.imwrite("research_current_variant9.png", variant9)
    results = reader.readtext(variant9)
    print("ğŸ§ª OCRåŸå§‹ç»“æœï¼ˆvariant9ï¼‰:", results)
    for _, text, conf in results:
        text_clean = text.replace(" ", "")
        if any(k in text_clean for k in ["ç ”ç©¶", "ç ”", "ç©¶"]) and conf > 0.08:
            print(f"ğŸ“˜ ã€ç ”ç©¶çŠ¶æ€ã€‘è¯†åˆ«æˆåŠŸ (variant9): {text}")
            return True

    # 3ï¸âƒ£ equalizeHist + adaptiveThreshold
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    eq = cv2.equalizeHist(gray)
    enhanced = cv2.adaptiveThreshold(eq, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                     cv2.THRESH_BINARY, 11, 2)
    cv2.imwrite("research_current_enhanced.png", enhanced)
    results = reader.readtext(enhanced)
    print("ğŸ§ª OCRåŸå§‹ç»“æœï¼ˆå¢å¼ºï¼‰:", results)
    for _, text, conf in results:
        text_clean = text.replace(" ", "")
        if any(k in text_clean for k in ["ç ”ç©¶", "ç ”", "ç©¶"]) and conf > 0.08:
            print(f"ğŸ“˜ ã€ç ”ç©¶çŠ¶æ€ã€‘è¯†åˆ«æˆåŠŸ (å¢å¼º): {text}")
            return True

    print("âŒ æœ€ç»ˆä¸‰é˜¶æ®µå‡æœªè¯†åˆ«å‡ºç ”ç©¶çŠ¶æ€")
    return False

def is_accel_available(img):
    region = crop(img, RESEARCH_POINTS["åŠ é€ŸæŒ‰é’®è¯†åˆ«åŒºåŸŸ"])  # ä¿ç•™å½©è‰²ï¼Œä¸è½¬ç°åº¦
    tpl_on = cv2.imread(IMG_ACCEL_TRUE_PATH)  # å½©è‰²è¯»å›¾
    tpl_off = cv2.imread(IMG_ACCEL_FALSE_PATH)  # å½©è‰²è¯»å›¾

    res_on = cv2.matchTemplate(region, tpl_on, cv2.TM_CCOEFF_NORMED)
    res_off = cv2.matchTemplate(region, tpl_off, cv2.TM_CCOEFF_NORMED)

    max_on = cv2.minMaxLoc(res_on)[1]
    max_off = cv2.minMaxLoc(res_off)[1]
    diff = max_on - max_off

    print(f"åŒ¹é…ç»“æœ â†’ äº®:{max_on:.3f} / ç°:{max_off:.3f} â†’ å·®å€¼:{diff:.3f}")

    # æ¨èåŠ å·®å€¼åˆ¤æ–­ï¼Œé¿å…äº®ç°åˆ†æ•°æ¥è¿‘æ—¶è¯¯åˆ¤
    return diff > 0.01 and max_on > 0.75


def detect_ke_yan_fa_with_easyocr(image):
    region = RESEARCH_POINTS["å¯ç ”å‘è¯†åˆ«åŒºåŸŸ"] 
    x1, y1, x2, y2 = region
    roi = image[y1:y2, x1:x2]

    # --- é€šç”¨å¢å¼ºæ–¹æ¡ˆï¼šç”¨ equalizeHist + adaptiveThreshold ---
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    equalized = cv2.equalizeHist(gray)
    enhanced = cv2.adaptiveThreshold(equalized, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                     cv2.THRESH_BINARY, 11, 2)
    #cv2.imwrite("ke_yan_fa_enhanced.png", enhanced)

    # --- ç¬¬ä¸€æ¬¡è¯†åˆ« ---
    print("ğŸ” æ­£åœ¨ç”¨ã€é€šç”¨å¢å¼ºæ–¹æ¡ˆã€‘å°è¯•è¯†åˆ« å¯ç ”å‘ ...")
    results = reader.readtext(enhanced)

    best_match = None
    best_conf = 0
    for bbox, text, conf in results:
        print(f"[é€šç”¨æ–¹æ¡ˆ] è¯†åˆ«: {text} (conf={conf:.2f})")
        if "å¯ç ”å‘" in text and conf > best_conf:
            best_conf = conf
            (bx1, by1), (bx2, by2) = bbox[0], bbox[2]
            x = int((bx1 + bx2) / 2) + x1
            y = int((by1 + by2) / 2) + y1
            best_match = (x, y)

    # --- å¦‚æœé€šç”¨æ–¹æ¡ˆè¯†åˆ«ä¸åˆ° â†’ fallback ç”¨æ–¹æ¡ˆ9 ---
    if best_match is None:
        print("âš ï¸ é€šç”¨å¢å¼ºæœªè¯†åˆ«åˆ° å¯ç ”å‘ï¼Œåˆ‡æ¢åˆ°ã€å¤‡ç”¨9ã€‘å†è¯• ...")
        variant9 = cv2.convertScaleAbs(roi, alpha=0.5, beta=0)
        #cv2.imwrite("ke_yan_fa_variant9.png", variant9)
        results = reader.readtext(variant9)

        for bbox, text, conf in results:
            print(f"[å¤‡ç”¨9] è¯†åˆ«: {text} (conf={conf:.2f})")
            if "å¯ç ”å‘" in text and conf > best_conf:
                best_conf = conf
                (bx1, by1), (bx2, by2) = bbox[0], bbox[2]
                x = int((bx1 + bx2) / 2) + x1
                y = int((by1 + by2) / 2) + y1
                best_match = (x, y)

    # --- è¿”å›ç»“æœ ---
    if best_match:
        print(f"âœ… ã€å¯ç ”å‘ã€‘è¯†åˆ«æˆåŠŸ: conf={best_conf:.2f}, åæ ‡={best_match}")
        return best_match
    else:
        print("âš ï¸ æœªè¯†åˆ«åˆ° å¯ç ”å‘ï¼Œä½¿ç”¨é»˜è®¤åæ ‡ç‚¹å‡»")
        return (537, 1408)

def try_click_accelerate():
    if _pause_callback: _pause_callback()
    print("â³ æ­£åœ¨æ£€æŸ¥åŠ é€ŸæŒ‰é’®çŠ¶æ€ï¼Œå‡†å¤‡è¿›å…¥ç§‘æŠ€é¡µé¢")
    server_socket.tap(*RESEARCH_POINTS["ä¸»é¡µ_æŠ€æœ¯æŒ‰é’®"])
    time.sleep(1.2)
    img = safe_get_screenshot()
    if img is None:
        print("âŒ è·å–æˆªå›¾å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¬¡åŠ é€Ÿæ£€æŸ¥")
        return
    if is_accel_available(img):
        print("âš¡ åŠ é€ŸæŒ‰é’®äº®èµ·ï¼Œç‚¹å‡»æ‰§è¡Œ")
        server_socket.tap(*RESEARCH_POINTS["åŠ é€ŸæŒ‰é’®"])
        time.sleep(0.8)
        server_socket.tap(*RESEARCH_POINTS["å…è´¹å‡å°‘æŒ‰é’®"])
        set_accelerate_timer(7199, callback=try_click_accelerate)
        if research_time_remaining and research_time_remaining > 1800:
            set_research_timer(research_time_remaining - 1800, callback=lambda: initialize_research_state(server_socket))
        else:
            set_research_timer(0, callback=lambda: initialize_research_state(server_socket))
    else:
        cd = extract_time(crop(img, RESEARCH_POINTS["åŠ é€ŸCDæ—¶é—´åŒºåŸŸ"]))
        if cd:
            set_accelerate_timer(cd, callback=try_click_accelerate)
        else:
            set_accelerate_timer(60, callback=try_click_accelerate)

def initialize_research_state(socket):
    if _pause_callback: _pause_callback()
    global research_running, server_socket, has_requested_help
    server_socket = socket
    research_ready_event.clear()
    has_requested_help = False
    start_timer_thread()

    server_socket.tap(*RESEARCH_POINTS["ä¸»é¡µ_æŠ€æœ¯æŒ‰é’®"])
    time.sleep(4)
    img = safe_get_screenshot()
    if img is None:
        print("âŒ æ— æ³•æˆªå›¾ï¼Œç»ˆæ­¢ç ”ç©¶æµç¨‹")
        research_ready_event.set()
        return

    if is_research_done(img):
        print("âœ… æ£€æµ‹åˆ°ç ”ç©¶å®Œæˆå›¾æ ‡ï¼Œè¿›å…¥æ–°ç ”ç©¶æµç¨‹")
        server_socket.tap(*RESEARCH_POINTS["å…³é—­ç ”ç©¶å®Œæˆé¡µé¢"])
        time.sleep(0.6)
        server_socket.tap(*detect_ke_yan_fa_with_easyocr(img))
        time.sleep(0.5)
        server_socket.tap(*RESEARCH_POINTS["ç§‘æŠ€ç ”ç©¶æŒ‰é’®"])
        time.sleep(0.8)
        if not has_requested_help:
            server_socket.tap(*RESEARCH_POINTS["è”ç›Ÿæ±‚åŠ©æŒ‰é’®"])
            has_requested_help = True
            time.sleep(0.5)
    else:
        if is_currently_researching(img):
            print("ğŸ“˜ å½“å‰å¤„äºç ”ç©¶ä¸­çŠ¶æ€")
            seconds = extract_time(crop(img, RESEARCH_POINTS["ç ”ç©¶å‰©ä½™æ—¶é—´åŒºåŸŸ"]))
            if seconds:
                print(f"â³ ç ”ç©¶å‰©ä½™æ—¶é—´ï¼š{seconds}ç§’")
                set_research_timer(seconds, callback=lambda: initialize_research_state(server_socket))
            if is_accel_available(img):
                print("âš¡ åŠ é€ŸæŒ‰é’®äº®èµ·ï¼Œç‚¹å‡»åŠ é€Ÿ")
                server_socket.tap(*RESEARCH_POINTS["åŠ é€ŸæŒ‰é’®"])
                time.sleep(0.5)
                server_socket.tap(*RESEARCH_POINTS["å…è´¹å‡å°‘æŒ‰é’®"])
                time.sleep(1.5)
                img = safe_get_screenshot()
                seconds = extract_time(crop(img, RESEARCH_POINTS["ç ”ç©¶å‰©ä½™æ—¶é—´åŒºåŸŸ"]))
                if seconds:
                    set_research_timer(seconds, callback=lambda: initialize_research_state(server_socket))
                set_accelerate_timer(7199, callback=try_click_accelerate)   
            cd = extract_time(crop(img, RESEARCH_POINTS["åŠ é€ŸCDæ—¶é—´åŒºåŸŸ"]))
            if cd:
                set_accelerate_timer(cd, callback=try_click_accelerate)
            else:
                set_accelerate_timer(60, callback=try_click_accelerate)
        else:
            print("ğŸ“Œ å½“å‰æœªæ£€æµ‹åˆ°ç ”ç©¶çŠ¶æ€ï¼Œé»˜è®¤è¿›å…¥æ–°ç ”ç©¶æµç¨‹")
            server_socket.tap(*RESEARCH_POINTS["å…³é—­ç ”ç©¶å®Œæˆé¡µé¢"])
            time.sleep(0.5)
            server_socket.tap(*detect_ke_yan_fa_with_easyocr(img))
            time.sleep(0.5)
            server_socket.tap(*RESEARCH_POINTS["ç§‘æŠ€ç ”ç©¶æŒ‰é’®"])
            time.sleep(0.5)
            if not has_requested_help:
                server_socket.tap(*RESEARCH_POINTS["è”ç›Ÿæ±‚åŠ©æŒ‰é’®"])
                has_requested_help = True
                time.sleep(0.5)

    for i in range(3):
        print(f"ğŸ” ç¬¬ {i + 1} æ¬¡å°è¯•è¿”å›ä¸»é¡µ...")
        server_socket.tap(*RESEARCH_POINTS["å…³é—­ç ”ç©¶é¡µé¢"])
        time.sleep(2.5)
        img = safe_get_screenshot()
        if img is not None:
            region = crop(img, RESEARCH_POINTS["ç ”ç©¶ä¸»é¡µåˆ¤æ–­åŒº"])
            if not match_template(region, "icons/YJZY-JS.png"):
                print("âœ… ç§‘æŠ€å›¾æ ‡å·²æ¶ˆå¤±ï¼ŒæˆåŠŸè¿”å›ä¸»é¡µ")
                research_ready_event.set()
                break
        else:
            print("â³ ä»åœ¨ç§‘æŠ€é¡µé¢ï¼Œç»§ç»­å°è¯•å…³é—­")

    research_running = True

def stop_research_monitor():
    global research_running
    research_running = False
